{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Keras\n",
    "\n",
    "Vamos usar Keras no conjunto de dados MNIST novamente, desta vez usando uma Rede Neural Convolucional que é mais adequada para o processamento de imagens. As CNNs são menos sensíveis a onde na imagem o padrão é o que procuramos.\n",
    "\n",
    "Com um perceptron multicamada, atingimos cerca de 97% de precisão. Vamos ver se podemos vencer isso.\n",
    "\n",
    "Como antes, começaremos importando as coisas que precisamos, incluindo os novos tipos de camadas de que falamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregaremos nosso conjunto de dados brutos exatamente como antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos moldar os dados de maneira diferente antes. Como estamos tratando os dados como imagens 2D de 28x28 pixels em vez de um fluxo achatado de 784 pixels, precisamos moldá-lo de acordo. Dependendo do formato de dados para o qual Keras está configurado, pode ser 1x28x28 ou 28x28x1 (o \"1\" indica um único canal de cor, pois é apenas escala de cinza. Se estivéssemos lidando com imagens coloridas, seria 3 em vez de 1 teríamos canais de cores vermelho, verde e azul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images /= 255\n",
    "test_images /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como antes, precisamos converter nossos rótulos de treino e de teste para serem categóricos em um formato \"one-hot\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como verificação de integridade, vamos imprimir uma das imagens de treinamento com o rótulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_sample(num):\n",
    "    #Print the one-hot array of this sample's label \n",
    "    print(train_labels[num])  \n",
    "    #Print the label converted back to a number\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    #Reshape the 768 values to a 28x28 image\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuração de uma rede neural convolucional envolve mais camadas. Nem todos estes são estritamente necessários; você pode executar sem pooling e dropout, mas essas etapas extras ajudam a evitar overfitting e ajudam as coisas a serem executadas mais rapidamente.\n",
    "\n",
    "Começaremos com uma convolução 2D da imagem - ela é configurada para tirar 32 janelas ou \"filtros\" de cada imagem, cada filtro com tamanho 3x3.\n",
    "\n",
    "Em seguida, executamos uma segunda convolução em cima disso com 64 janelas 3x3 - essa topologia é exatamente o que é recomendado nos exemplos do próprio Keras. Mais uma vez, você quer reutilizar pesquisas anteriores sempre que possível enquanto ajusta as CNNs, já que é difícil fazer isso.\n",
    "\n",
    "Em seguida, aplicamos uma camada MaxPooling2D que leva o máximo de cada resultado 2x2 para destilar os resultados em algo mais gerenciável.\n",
    "\n",
    "Um filtro de dropout é então aplicado para evitar overfitting.\n",
    "\n",
    "Em seguida, nivelamos a camada 2D que temos neste estágio em uma camada 1D. Então, neste ponto, podemos apenas fingir que temos um perceptron multicamada tradicional ...\n",
    "\n",
    "... e alimentar isso em uma camada oculta e plana de 128 unidades.\n",
    "\n",
    "Em seguida, aplicamos o abandono novamente para evitar o overfitting.\n",
    "\n",
    "E finalmente, nós alimentamos isso em nossas 10 unidades finais onde softmax é aplicado para escolher nossa categoria de 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "# 64 3x3 kernels\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Reduce by taking the max of each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Dropout to avoid overfitting\n",
    "model.add(Dropout(0.25))\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "# A hidden layer to learn with\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Another dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Final categorization from 0-9 with softmax\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar novamente a descrição do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda estamos fazendo várias categorizações, então categorical_crossentropy ainda é a função de perda correta a ser usada. Usaremos o otimizador Adam, embora o exemplo fornecido com o Keras use o RMSProp. Você pode querer tentar os dois se você tiver tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora nós treinamos nosso modelo ... para tornar as coisas um pouco mais rápidas, usaremos lotes de 32.\n",
    "\n",
    "## Aviso\n",
    "\n",
    "Isso pode levar horas para ser executado, e a CPU do seu computador será excedida durante esse tempo! Não execute o próximo bloco, a menos que você possa dedicar seu computador por um longo tempo. Ele irá imprimir o progresso conforme cada época é executada, mas cada época pode levar cerca de 20 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 113s - loss: 0.1884 - acc: 0.9428 - val_loss: 0.0457 - val_acc: 0.9841\n",
      "Epoch 2/10\n",
      " - 117s - loss: 0.0809 - acc: 0.9760 - val_loss: 0.0319 - val_acc: 0.9895\n",
      "Epoch 3/10\n",
      " - 130s - loss: 0.0604 - acc: 0.9811 - val_loss: 0.0310 - val_acc: 0.9897\n",
      "Epoch 4/10\n",
      " - 104s - loss: 0.0480 - acc: 0.9851 - val_loss: 0.0344 - val_acc: 0.9895\n",
      "Epoch 5/10\n",
      " - 100s - loss: 0.0425 - acc: 0.9868 - val_loss: 0.0323 - val_acc: 0.9901\n",
      "Epoch 6/10\n",
      " - 99s - loss: 0.0360 - acc: 0.9892 - val_loss: 0.0317 - val_acc: 0.9904\n",
      "Epoch 7/10\n",
      " - 99s - loss: 0.0307 - acc: 0.9908 - val_loss: 0.0266 - val_acc: 0.9917\n",
      "Epoch 8/10\n",
      " - 99s - loss: 0.0284 - acc: 0.9909 - val_loss: 0.0332 - val_acc: 0.9914\n",
      "Epoch 9/10\n",
      " - 98s - loss: 0.0257 - acc: 0.9920 - val_loss: 0.0309 - val_acc: 0.9919\n",
      "Epoch 10/10\n",
      " - 99s - loss: 0.0247 - acc: 0.9924 - val_loss: 0.0331 - val_acc: 0.9914\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valeu a pena esperar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.034049834153382426\n",
      "Test accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais de 99%! E isso é com apenas 10 épocas! Ele teve um custo significativo em termos de poder de computação, mas quando você começa a distribuir coisas em vários computadores, cada um com várias GPUs, esse custo começa a parecer menos ruim. Se você está construindo algo em que a vida e a morte estão em jogo, como um carro autônomo, cada fração de um percentual é importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
